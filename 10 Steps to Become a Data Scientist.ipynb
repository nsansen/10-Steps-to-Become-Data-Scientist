{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f550639a0754f42a243e5785d895d24ba655515"
   },
   "source": [
    "\n",
    "<img src=\"http://s9.picofile.com/file/8338833934/DS.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e02d495da0fb0ad24e0341e91848f4c4cfc35bdb"
   },
   "source": [
    "\n",
    "\n",
    "---------------------------------------------------------------------\n",
    "Fork and Run this kernel on GitHub:\n",
    "> ###### [ GitHub](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    " **I hope you find this kernel helpful and some UPVOTES would be very much appreciated**\n",
    " \n",
    " -----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "85b27cf82d3023fd69c338df2be7afb2d7afaf32"
   },
   "source": [
    " <a id=\"0\"></a> <br>\n",
    "**Notebook Content**\n",
    "1. [Python](#1)\n",
    "1. [Python Packages](#11)\n",
    "1. [Mathematics and Linear Algebra](#46)\n",
    "1. [Programming & Analysis Tools](#47)\n",
    "1. [Big Data](#49)\n",
    "1. [Data visualization](#50)\n",
    "1. [Data Cleaning](#51)\n",
    "1. [How to solve Problem?](#52)\n",
    "1. [Machine Learning](#53)\n",
    "1. [Deep Learning](#54)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ada06bdafb4dbf2d86d81da323000aa7999b3344"
   },
   "source": [
    " ## <div align=\"center\">  10 Steps to Become a Data Scientist</div>\n",
    " <div align=\"center\">**quite practical and far from any theoretical concepts**</div>\n",
    "<div style=\"text-align:center\">last update: <b>10/16/2018</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Introduction\n",
    "If you Read and Follow Job Ads to hire a machine learning expert or a data scientist, you find that some skills you should have to get the job.\n",
    "\n",
    "In this Kernel, we want to review **10 skills** that are essentials to get the job\n",
    "\n",
    "in fact, this kernel is a reference for **ten other kernels**, which you can learn with all the necessary skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5efeff35ad9951e40551d0763eaf26f08bb4119e"
   },
   "source": [
    " <a id=\"1\"></a> <br>\n",
    "# 1-Python\n",
    "\n",
    "for Reading this section **please** fork and upvote  this kernel:\n",
    "\n",
    "[numpy-pandas-matplotlib-seaborn-scikit-learn](https://www.kaggle.com/mjbahmani/numpy-pandas-matplotlib-seaborn-scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a8697f93952e076f6f949997676d40518d7b5a6"
   },
   "source": [
    "<a id=\"11\"></a> <br>\n",
    "# 2-Python Packages\n",
    "* Numpy\n",
    "* Pandas\n",
    "* Matplotlib\n",
    "* Seaborn\n",
    "\n",
    "for Reading this section **please** fork and upvote  this kernel:\n",
    "\n",
    "[numpy-pandas-matplotlib-seaborn-scikit-learn](https://www.kaggle.com/mjbahmani/numpy-pandas-matplotlib-seaborn-scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4524fb4d3bdeca34f1a526bbf2fc2caa9bfbf51a",
    "collapsed": true
   },
   "source": [
    "<a id=\"44\"></a> <br>\n",
    "## 2-4 SKlearn\n",
    "\n",
    "for Reading this section **please** fork and upvote  this kernel:\n",
    "\n",
    "[A-Journey-with-scikit-learn](https://www.kaggle.com/mjbahmani/a-journey-with-scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad8fa54ba57aa4a336080eb044109702c743d7a0"
   },
   "source": [
    "<a id=\"45\"></a> <br>\n",
    "##  3- Mathematics and Linear Algebra\n",
    "\n",
    "for Reading this section **please** fork and upvote  this kernel:\n",
    "\n",
    "[Linear Algebra in 60 Minutes](https://www.kaggle.com/mjbahmani/linear-algebra-in-60-minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "697ba206ad7adf4d99814cb1d89375b745eaba19"
   },
   "source": [
    "<a id=\"46\"></a> <br>\n",
    "## 4- Programming & Analysis Tools\n",
    "\n",
    "* RapidMiner:\n",
    "\n",
    "RapidMiner (RM) was originally started in 2006 as an open-source stand-alone software named Rapid-I. Over the years, they have given it the name of RapidMiner and also attained ~35Mn USD in funding. The tool is open-source for old version (below v6) but the latest versions come in a 14-day trial period and licensed after that.\n",
    "\n",
    "RM covers the entire life-cycle of prediction modeling, starting from data preparation to model building and finally validation and deployment. The GUI is based on a block-diagram approach, something very similar to Matlab Simulink. There are predefined blocks which act as plug and play devices. You just have to connect them in the right manner and a large variety of algorithms can be run without a single line of code. On top of this, they allow custom R and Python scripts to be integrated into the system.\n",
    "\n",
    "There current product offerings include the following:\n",
    "\n",
    "RapidMiner Studio: A stand-alone software which can be used for data preparation, visualization and statistical modeling\n",
    "RapidMiner Server: It is an enterprise-grade environment with central repositories which allow easy team work, project management and model deployment\n",
    "RapidMiner Radoop: Implements big-data analytics capabilities centered around Hadoop\n",
    "RapidMiner Cloud: A cloud-based repository which allows easy sharing of information among various devices\n",
    "RM is currently being used in various industries including automotive, banking, insurance, life Sciences, manufacturing, oil and gas, retail, telecommunication and utilities.\n",
    "\n",
    "* DataRobot:\n",
    "\n",
    "DataRobot (DR) is a highly automated machine learning platform built by all time best Kagglers including Jeremy Achin, Thoman DeGodoy and Owen Zhang. Their platform claims to have obviated the need for data scientists. This is evident from a phrase from their website – “Data science requires math and stats aptitude, programming skills, and business knowledge. With DataRobot, you bring the business knowledge and data, and our cutting-edge automation takes care of the rest.”\n",
    "\n",
    "DR proclaims to have the following benefits:\n",
    "\n",
    "Model Optimization\n",
    "Platform automatically detects the best data pre-processing and feature engineering by employing text mining, variable type detection, encoding, imputation, scaling, transformation, etc.\n",
    "Hyper-parameters are automatically chosen depending on the error-metric and the validation set score\n",
    "Parallel Processing\n",
    "Computation is divided over thousands of multi-core servers\n",
    "Uses distributed algorithms to scale to large data sets\n",
    "Deployment\n",
    "Easy deployment facilities with just a few clicks (no need to write any new code)\n",
    "For Software Engineers\n",
    "Python SDK and APIs available for quick integration of models into tools and softwares.\n",
    "\n",
    "* BigML:\n",
    "\n",
    "BigML provides a good GUI which takes the user through 6 steps as following:\n",
    "\n",
    "Sources: use various sources of information\n",
    "Datasets: use the defined sources to create a dataset\n",
    "Models: make predictive models\n",
    "Predictions: generate predictions based on the model\n",
    "Ensembles: create ensemble of various models\n",
    "Evaluation: very model against validation sets\n",
    "These processes will obviously iterate in different orders. The BigML platform provides nice visualizations of results and has algorithms for solving classification, regression, clustering, anomaly detection and association discovery problems. They offer several packages bundled together in monthly, quarterly and yearly subscriptions. They even offer a free package but the size of the dataset you can upload is limited to 16MB.\n",
    "\n",
    "* Google Cloud AutoML:\n",
    "\n",
    "Cloud AutoML is part of Google’s Machine Learning suite offerings that enables people with limited ML expertise to build high quality models. The first product, as part of the Cloud AutoML portfolio, is Cloud AutoML Vision. This service makes it simpler to train image recognition models. It has a drag-and-drop interface that let’s the user upload images, train the model, and then deploy those models directly on Google Cloud.\n",
    "\n",
    "Cloud AutoML Vision is built on Google’s transfer learning and neural architecture search technologies (among others). This tool is already being used by a lot of organizations. Check out this article to see two amazing real-life examples of AutoML in action, and how it’s producing better results than any other tool.\n",
    "\n",
    "* Paxata:\n",
    "\n",
    "Paxata is one of the few organizations which focus on data cleaning and preparation, and not the machine learning or statistical modeling part. It is an MS Excel-like application that is easy to use. It also provides visual guidance making it easy to bring together data, find and fix dirty or missing data, and share and re-use data projects across teams. Like the other tools mentioned in this article, Paxata eliminates coding or scripting, hence overcoming technical barriers involved in handling data.\n",
    "\n",
    "Paxata platform follows the following process:\n",
    "\n",
    "Add Data: use a wide range of sources to acquire data\n",
    "Explore: perform data exploration using powerful visuals allowing the user to easily identify gaps in data\n",
    "Clean+Change: perform data cleaning using steps like imputation, normalization of similar values using NLP, detecting duplicates\n",
    "Shape: make pivots on data, perform grouping and aggregation\n",
    "Share+Govern: allows sharing and collaborating across teams with strong authentication and authorization in place\n",
    "Combine: a proprietary technology called SmartFusion allows combining data frames with 1 click as it automatically detects the best combination possible; multiple data sets can be combined into a single AnswerSet\n",
    "BI Tools: allows easy visualization of the final AnswerSet in commonly used BI tools; also allows easy iterations between data preprocessing and visualization\n",
    "Praxata has set its foot in financial services, consumer goods and networking domains. It might be a good tool to use if your work requires extensive data cleaning.\n",
    "\n",
    "* Microsoft Azure ML Studio\n",
    "\n",
    "When there are so many big name players in this field, how could Microsoft lag behind? The Azure ML Studio is a simple yet powerful browser based ML platform. It has a visual drag-and-drop environment where there is no requirement of coding. They have published comprehensive tutorials and sample experiments for newcomers to get the hang of the tool quickly. It employs a simple five step process:\n",
    "\n",
    "Import your dataset\n",
    "Perform data cleaning and other preprocessing steps, if necessary\n",
    "Split the data into training and testing sets\n",
    "Apply built-in ML algorithms to train your model\n",
    "Score your model and get your predictions!\n",
    "* Amazon Lex:\n",
    "\n",
    "Amazon Lex provides an easy-to-use console for building your own chatbot in a matter of minutes. You can build conversational interfaces in your applications or website using Lex. All you need to do is supply a few phrases and Amazon Lex does the rest! It builds a complete Natural Language model using which a customer can interact with your app, using both voice and text.\n",
    "\n",
    "It also comes with built-in integration with the Amazon Web Services (AWS) platform. Amazon Lex is a fully managed service so as your user engagement increases, you don’t need to worry about provisioning hardware and managing infrastructure to improve your bot experience.\n",
    "\n",
    "In this section, we have discussed various initiatives working towards automating various aspects of solving a data science problem. Some of them are in a nascent research stage, some are open-source and others are already being used in the industry with millions in funding. All of these pose a potential threat to the job of a data scientist, which is expected to grow in the near future. These tools are best suited for people who are not familiar with programming & coding.\n",
    "\n",
    "Do you know any other startups or initiatives working in this domain? Please feel free to drop a comment below and enlighten us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00f5c5ce80c7e302e83f0ea9b451dfaae7aa52cf"
   },
   "source": [
    "## 5- Big Data\n",
    "\n",
    "for Reading this section **please** fork and upvote  this kernel:\n",
    "\n",
    "[A-Comprehensive-Deep-Learning-Workflow-with-Python](https://www.kaggle.com/mjbahmani/a-comprehensive-deep-learning-workflow-with-python)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33bb9c265bef5e4474dcac0638cc632b5532f1ce"
   },
   "source": [
    "## 6- Data Visualization\n",
    "we will release the full version of Deep Learning **Coming Soon**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9bf1d9444651e2756c4fa4d71914ec20d621305e"
   },
   "source": [
    "## 7- Data Cleaning\n",
    "we will release the full version of Deep Learning **Coming Soon**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8720a4ddaab64e4bff226bed9e4e200dc9b94913"
   },
   "source": [
    "## 8- How to solve Problem?\n",
    "**Data Science has so many techniques and procedures that can confuse anyone.**\n",
    "\n",
    "**Step 1**: Translate your business problem statement into technical one\n",
    "\n",
    "Analogous to any other software problem, data science aims at solving a business problem. Most of the times, business problem statements are vague and can be interpreted in multiple ways. This occurs mostly because we generally use qualitative words in our language which cannot be directly translated into a machine readable code.\n",
    "\n",
    "Eg. Let’s say we need to develop a solution to reduce crime rate of a city. The term “reduce” can be interpreted as:\n",
    "\n",
    "Decreasing crime rate of areas with high crime rate\n",
    "Decreasing crime rate of the most common type of crime\n",
    "It is a good practice to circle back with the client or the business team who define the problem to decide on the right interpretation.\n",
    "\n",
    "**Step 2**: Decide on the supervised learning technique\n",
    "\n",
    "The end goal of almost every data science problem is usually classification or regression. Deciding the supervised technique for the problem will help you get more clarity on the business statement.\n",
    "\n",
    "Eg. Let’s look at our problem of reducing crime rate. While the problem of reducing crime rate is more of a policy decision, depending on the choice above, we would have to decide if we need to do classification or regression.\n",
    "\n",
    "If we need to decrease crime rate of areas with high crime rate, we would need to determine the crime rate rate of an area. This is a regression problem.\n",
    "If we need to decrease crime rate of most common type of crime, we would need to determine the most common type of crime in an area. This is a classification problem.\n",
    "Again it is a good practice to circle back with the client or the business team who define the problem requirements to clarify on the exact requirement.\n",
    "\n",
    "**Step 3**: Literature survey\n",
    "\n",
    "Literature Survey is one of the most important step (and often most ignored step) to approach any problem. If you read any article about components of Data Science, you will find computer science, statistics / math and domain knowledge. As it is quite inhuman for someone to have subject expertise in all possible fields, literature survey can often help in bridging the gaps of inadequate subject expertise.\n",
    "\n",
    "After going through existing literature related to a problem, I usually try to come up with a set of hypotheses that could form my potential set of features. Going through existing literature helps you understand existing proofs in the domain serving as a guide to take the right direction in your problem. It also helps in interpretation of the results obtained from the prediction models.\n",
    "\n",
    "Eg. Going back to our problem of reducing crime rate, if you want to predict crime rate of an area, you would consider factors from general knowledge like demographics, neighboring areas, law enforcement rules etc. Literature survey will help you consider additional variables like climate, mode of transportation, divorce rate etc.\n",
    "\n",
    "**Step 4**: Data cleaning\n",
    "\n",
    "If you speak with anyone who has spent some time in data science, they will always say that most of their time is spent on cleaning the data. Real world data is always messy. Here are a few common discrepancies in most data-sets and some techniques of how to clean them:\n",
    "\n",
    "Missing values\n",
    "Missing values are values that are blank in the data-set. This can be due to various reasons like value being unknown, unrecorded, confidential etc. Since the reason for a value being missing is not clear, it is hard to guess the value.\n",
    "\n",
    "You could try different techniques to impute missing values starting with simple methods like column mean, median etc. and complex methods like using machine leaning models to estimate missing values.\n",
    "\n",
    "Duplicate records\n",
    "The challenge with duplicate records is identifying a record being duplicate. Duplicate records often occur while merging data from multiple sources. It could also occur due to human error. To identify duplicates, you could approximate a numeric values to certain decimal places and for text values, fuzzy matching could be a good start. Identification of duplicates could help the data engineering team to improve collection of data to prevent such errors.\n",
    "\n",
    "Incorrect values\n",
    "Incorrect values are mostly due to human error. For Eg. If there is a field called age and the value is 500, it is clearly wrong. Having domain knowledge of the data will help identify such values. A good technique to identify incorrect values for numerical columns could be to manually look at values beyond 3 standard deviations from the mean to check for correctness.\n",
    "\n",
    "**Step 5**: Feature engineering\n",
    "\n",
    "Feature Engineering is one of the most important step in any data science problem. Good set of features might make simple models work for your data. If features are not good enough, you might need to go for complex models. Feature Engineering mostly involves:\n",
    "\n",
    "Removing redundant features\n",
    "If a feature is not contributing a lot to the output value or is a function of other features, you can remove the feature. There are various metrics like AIC and BIC to identify redundant features. There are built in packages to perform operations like forward selection, backward selection etc. to remove redundant features.\n",
    "\n",
    "Transforming a feature\n",
    "A feature might have a non linear relationship with the output column. While complex models can capture this with enough data, simple models might not be able to capture this. I usually try to visualize different functions of each column like log, inverse, quadratic, cubic etc. and choose the transformation that looks closest to a normal curve.\n",
    "\n",
    "**Step 6**: Data modification\n",
    "\n",
    "Once the data is cleaned, there are a few modifications that might be needed before applying machine learning models. One of the most common modification would be scaling every column to the same range in order to give same weight to all columns. Some of the other required modifications might be data specific Eg. If output column is skewed, you might need to up-sample or down-sample.\n",
    "\n",
    "Steps 7 through 9 are iterative.\n",
    "\n",
    "**Step 7**: Modelling\n",
    "\n",
    "Once I have the data ready, I usually start with trying all the standard machine learning models. If it is a classification problem, a good start will beLogistic Regression, Naive Bayes, k-Nearest Neighbors, Decision Tree etc. If it is a regression problem, you could try linear regression, regression tree etc. The reason for starting with simple models is that simple models have lesser parameters to alter. If we start with a complex model like Neural Network orSupport Vector Machines, there are so many parameters that you could change that trying all options exhaustively might be time consuming.\n",
    "\n",
    "Each of the machine learning models make some underlying assumptions about the data. For Eg. Linear Regression / Logistic Regression assumes that the data comes from a linear combination of input parameters. Naive Bayes makes an assumption that the input parameters are independent of each other. Having the knowledge of these assumptions can help you judge the results of the different models. It is often helpful to visualize the actual vs predicted values to see these differences.\n",
    "\n",
    "**Step 8**: Model comparison\n",
    "\n",
    "One of the most standard technique to evaluate different machine learning models would be through the process of cross validation. I usually choose 10-fold cross validation but you may choose the right cross validation split based on the size of the data. Cross validation basically brings out an average performance of a model. This can help eliminate choosing a model that performs good specific to the data or in other words avoid over-fitting. It is often a good practice to randomize data before cross validation.\n",
    "\n",
    "A good technique to compare performance of different models is ROC curves. ROC curves help you visualize performance of different models across different thresholds. While ROC curves give a holistic sense of model performance, based on the business decision, you must choose the performance metric like Accuracy, True Positive Rate, False Positive Rate, F1-Score etc.\n",
    "\n",
    "**Step 9**: Error analysis\n",
    "\n",
    "At this point, you have tried a bunch of machine learning models and got the results. It is a good usage of time to not just look at the results like accuracy or True Positive Rate but to look at the set of data points that failed in some of the models. This will help you understand the data better and improve the models faster than trying all possible combinations of models. This is the time to try ensemble models like Random Forest, Gradient Boosting or a meta model of your own [Eg. Decision tree + Logistic Regression]. Ensemble models are almost always guaranteed to perform better than any standard model.\n",
    "\n",
    "**Step 10**: Improving your best model\n",
    "\n",
    "Once I have the best model, I usually plot training vs testing accuracy [or the right metric] against the number of parameters. Usually, it is easy to check training and testing accuracy against number of data points. Basically this plot will tell you whether your model is over-fitting or under-fitting. This articleDetecting over-fitting vs under-fitting explains this concept clearly.\n",
    "\n",
    "Understanding if your model is over-fitting or under-fitting will tell you how to proceed with the next steps. If the model is over-fitting, you might consider collecting more data. If the model is under-fitting, you might consider making the models more complex. [Eg. Adding higher order terms to a linear / logistic regression]\n",
    "\n",
    "**Step 11**: Deploying the model\n",
    "\n",
    "Once you have your final model, you would want the model to be deployed so that it automatically predicts output for new data point without retraining. While you can derive a formula for simple models like Linear Regression, Logistic Regression, Decision Tree etc. , it is not so straight forward for complex models like SVM, Neural Networks, Random Forest etc. I’m not very familiar with other languages but Python has a library called pickle which allows you to save models and use it to predict output for new data.\n",
    "\n",
    "**Step 12**: Adding feedback\n",
    "\n",
    "Usually, data for any data science problem is historical data. While this might be similar to the current data up-to a certain degree, it might not be able to capture the current trends or changes. For Eg. If you are using population as an input parameter, while population from 2015–2016 might vary slightly, if you use the model after 5 years, it might give incorrect results.\n",
    "\n",
    "One way to deal with this problem is to keep retraining your model with additional data. This might be a good option but retraining a model might be time consuming. Also, if you have applications in which data inflow is huge, this might need to be done at regular intervals. An alternative and a better option would be to use active learning. Active learning basically tries to use real time data as feedback and automatically update the model. The most common approaches to do this are Batch Gradient Descent and Stochastic Gradient Descent. It might be appropriate to use the right approach based on the application.\n",
    "\n",
    "Concluding remarks\n",
    "\n",
    "The field of data science is really vast. People spend their lifetime researching on individual topics discussed above. As a data scientist, you would mostly have to solve business problems than researching on individual subtopics. Additionally, you will have to explain the technical process and results to business teams who might not have enough technical knowledge. Thus, while you might not need a very in-depth knowledge of every technique, you need to have enough clarity to abstract the technical process and results and explain it in business terms.[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d4f8718cc7e1a8fc60a3815b55a2ab9a5eeef4f9"
   },
   "source": [
    "## 9- Machine learning  \n",
    "for Reading this section **please** fork and upvote  this kernel:\n",
    "\n",
    "[A Comprehensive ML Workflow with Python](http://https://www.kaggle.com/mjbahmani/a-comprehensive-ml-workflow-with-python)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3544d2fd1490f646f2f1c0fd4271f9a8745d2e36"
   },
   "source": [
    "##  10- Deep Learning\n",
    "\n",
    "for Reading this section **please** fork and upvote  this kernel:\n",
    "\n",
    "[A-Comprehensive-Deep-Learning-Workflow-with-Python](https://www.kaggle.com/mjbahmani/a-comprehensive-deep-learning-workflow-with-python)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3065412feed4f072e90154bb3eaed0fc3504d88d",
    "collapsed": true
   },
   "source": [
    "## References:\n",
    "1. [Coursera](https://www.coursera.org/specializations/data-science-python)\n",
    "2. [Hands-On Machine Learning with Scikit-Learn and TensorFlow](http://shop.oreilly.com/product/0636920052289.do)\n",
    "3. [How to solve Problem](https://www.linkedin.com/pulse/how-i-approach-data-science-problem-ganesh-n-prasad/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "edb768e0b3390ec29acab20593948c3f3bbf5bba",
    "collapsed": true
   },
   "source": [
    "---------------------------------------------------------------------\n",
    "Fork and Run this kernel on GitHub:\n",
    "> ###### [ GitHub](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n",
    "\n",
    " \n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    " **I hope you find this kernel helpful and some <font color=\"red\">UPVOTES</font> would be very much appreciated**\n",
    " \n",
    " -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e07313484155d573b97e7d21e6be7a60dc6768e3",
    "collapsed": true
   },
   "source": [
    "# Not completed yet!!!\n",
    "\n",
    "**Update every two days**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
